{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir('e:\\\\coding')\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import system\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score,StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "b=[]\n",
    "acc = []\n",
    "F_score =[]\n",
    "names=[]\n",
    "trial=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title(i):\n",
    "    name=\"\"\n",
    "    for e in b[i]:\n",
    "        name= e+\".\"+name\n",
    "    return(name)\n",
    "\n",
    "def get_score(model, x_train, x_test, y_train, y_test):\n",
    "    model.fit(x_train, y_train)\n",
    "    return model.score(x_test, y_test)\n",
    "\n",
    "def get_average(scores_mlp):\n",
    "    average_accuracy=np.mean(scores_mlp)\n",
    "    return average_accuracy\n",
    "def performance_measurment(cm):\n",
    "   \n",
    "    FP = cm.sum(axis=1) - np.diag(cm) #FP-False Positive \n",
    "    FN = cm.sum(axis=0) - np.diag(cm) #FN-False Negatives\n",
    "    TP = np.diag(cm)                  #TP-True Positive\n",
    "    TN = cm.sum() - (FP + FN + TP)    #TN-True Negatives\n",
    "\n",
    "    FP = FP.astype(float)\n",
    "    FN = FN.astype(float)\n",
    "    TP = TP.astype(float)\n",
    "    TN = TN.astype(float)\n",
    "\n",
    "    ACC1 = np.round((TP+TN)/(TP+FP+FN+TN),decimals=2) #Accuracy\n",
    "    precision1 = np.round((TP / (TP + FP)),decimals=2) #Precision\n",
    "    recall1 = np.round((TP / (TP + FN)),decimals=2)   #Recall\n",
    "    F1_SCORE1 = np.round((2*recall1*precision1)/(recall1+precision1),decimals=2) #F1-Score\n",
    "\n",
    "    \n",
    "    return ACC1, precision1, recall1, F1_SCORE1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"final_data.csv\"\n",
    "df = pd.read_csv(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features =[2,3,4,7,8,9,10,13]\n",
    "for L in range(1,len(features) + 1):\n",
    "    for subset in itertools.combinations(features, L):\n",
    "        a.append(list(subset))\n",
    "\n",
    "\n",
    "features = ['mean(ni)','median(ni)','standard_deviation(ni)','range(ni)','mean(nid)','median(nid)','standard_deviation(nid)','range(nid)']\n",
    "for L in range(1,len(features) + 1):\n",
    "    for subset in itertools.combinations(features, L):\n",
    "        b.append(list(subset))\n",
    "\n",
    "le = LabelEncoder()\n",
    "sc = StandardScaler()\n",
    "df['polymer'] = le.fit_transform(df[\"polymer\"])\n",
    "df[df.columns[2:]]=sc.fit_transform(df[df.columns[2:]])\n",
    "Y = df[df.columns[1]].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "\n",
    "    \n",
    "    name =title(i)\n",
    "    X = df[df.columns[a[i]]].values\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=1, shuffle=True, stratify=Y)\n",
    "\n",
    "\n",
    "    mlp = MLPClassifier(max_iter=200, random_state=0, verbose=True, activation='relu', batch_size=32, learning_rate_init= 0.0001,\n",
    "    n_iter_no_change=200,hidden_layer_sizes=(512,256, 128, 64,32,16), solver='adam',shuffle=True, early_stopping=True)\n",
    "\n",
    "    kf = KFold(5, shuffle=True, random_state=42) # Use for KFold classification\n",
    "    scores_mlp = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        x_train, x_test, y_train, y_test = X[train_index], X[test_index],Y[train_index], Y[test_index]\n",
    "        scores_mlp.append(get_score(mlp, x_train, x_test, y_train, y_test)) \n",
    "\n",
    "    get_average(scores_mlp)\n",
    "\n",
    "    \n",
    "    y_pred = mlp.predict(x_test)\n",
    "    precision=round(precision_score(y_test, y_pred,average='weighted'),2)\n",
    "    recall=round(recall_score(y_test, y_pred,average='weighted'),2)\n",
    "    accuracy=round(accuracy_score(y_test, y_pred),2)\n",
    "\n",
    "    F1_SCORE=round(f1_score(y_test, y_pred,average='weighted'),2)\n",
    "\n",
    "    cm = confusion_matrix(y_pred, y_test)\n",
    "\n",
    "    ACC1, precision1, recall1, F1_SCORE1 = performance_measurment(cm)\n",
    "    degrees = 35\n",
    "    names.append(name)\n",
    "    acc.append(accuracy)\n",
    "    F_score.append(F1_SCORE)\n",
    "\n",
    "    class_names = [\" PA \", \" PC \", \" PE \", \" PP \", \" PS \",\" POM \",\" PVC \",\" PET \",\" PU \",\" ABS \",\" PE-LD \",\" SAN \"]\n",
    "    plot_confusion_matrix(mlp, x_test, y_test,display_labels=class_names,xticks_rotation=degrees,cmap=plt.cm.OrRd)\n",
    "    plt.title(\"MLP_Classifier Combination:\" + str(name))\n",
    "    # print(\"Combination:\" + str(name) +\"  Accuracy: \" + str(accuracy) +  \"\\nPrecision: \"+ str(precision)+\"  Recall: \"+ str(recall)  + \"  F1-SCORE: \"+ str(F1_SCORE))\n",
    "    # print(\"Combination:\" + str(name) +\"  Accuracy_Matrix: \" + str(ACC1) +  \"\\nPrecision_Matrix: \"+ str(precision1) +\"\\n  Recall_Matrix: \"+ str(recall1) + \"\\n F1-SCORE_Matrix: \"+ str(F1_SCORE1))\n",
    "\n",
    "    plt.savefig(name+\"png\")\n",
    "     \n",
    "    print(\"number of trials completed = \", trial)\n",
    "    trial+=1\n",
    "\n",
    "\n",
    "data=pd.DataFrame({'Name':names,'Accuracy':acc,'F1_score':F_score})\n",
    "data.to_excel('final.xlsx',index=True,header=True)\n",
    "print('process complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "63963b3f4c440940f0b94a3100916033a226cb4f45979123153792d60aa56d6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
